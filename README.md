# AIGC 算法工程师经典百问

> 本人是某双一流大学硕士生，也最近刚好准备参加 2024年秋招，在找大模型算法岗实习中，遇到了很多有意思的面试，所以将这些面试题记录下来，并分享给那些和我一样在为一份满意的offer努力着的小伙伴们！！！

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/goiboxqfW2fYU4UrnNhiaS7UvNxic6vibCqicPcNnNQic02zO1ia6ZhrkRbvZfltWe8EUvibOIcNIIoSNmqx0Qs09xGghg/640?wx_fmt=jpeg&from=appmsg&wxfrom=13)

- [transformer里PE为什么不采用concatenation的方式？](https://mp.weixin.qq.com/s/JYuBoQnTQDYOnNDwFGzvFQ)
- [讲一下PostNorm 和 PreNorm？这两个有什么优缺点？](https://mp.weixin.qq.com/s/6JRDQq1fCHqF4dgEywSmxg)
- [大模型SFT不就好了，为什么还要RLHF？](https://mp.weixin.qq.com/s/-ovjctqf0jGBE4sX7ld4rw)
- [阿里面试官问：为什么vllm能够加快大模型推理速度？](https://mp.weixin.qq.com/s/xS80-E_a_4_VSF9nYeJhPg)
- [BERT中的多头注意力机制-为什么需要多头？](https://mp.weixin.qq.com/s/_c4WkTbUcxAiEcYO61oUTg)
- [阿里面试官问：什么是softmax 上下溢出问题？](https://mp.weixin.qq.com/s/Tfp7ZT0haYJxnO_sIxO5fg)
- [为什么NLP用Layernorm而不是batchnorm？](https://mp.weixin.qq.com/s/k2SEZkmSbzsKhRLhFsEFIQ)
- [NLP 经典面试题：RLHF 训练过程是怎么样的？DPO 如何解决RLHF存在问题？](https://articles.zsxq.com/id_2ivk85apuunw.html)
- [在PyTorch框架中model.train() 和 model.eval()的作用是什么？](https://articles.zsxq.com/id_100820huslws.html)
- [大模型经典面试题————如何解决大模型推理过程中的延迟问题？](https://articles.zsxq.com/id_n4kvdwli1ueq.html)
- [NLP 经典面试题————Transformer中 前馈层（FFN） 的作用是什么？](https://articles.zsxq.com/id_jpg9ipkj8fdw.html)
- [LLMs经典面试题————为什么KV Cache没有Q-Cache？](https://articles.zsxq.com/id_e2a1d3mksi47.html)
- [面试官问：深度网络中loss除以10和学习率除以10等价吗](https://articles.zsxq.com/id_b3gveghpd7ho.html)
- [大模型面试题——大模型部署框架对比](https://articles.zsxq.com/id_w2jlrhe971dk.html)
- [学妹问：硕士校招生进入大模型领域工作，选预训练还是SFT？](https://articles.zsxq.com/id_zwsiaghwd7fl.html)
- [阿里面试官：Transformers 中的 Softmax 可以并行加速么？](https://articles.zsxq.com/id_9okq707vnjpa.html)
- [阿里面试官问：Self-Attention 的时间复杂度/空间复杂度是怎么计算的?](https://articles.zsxq.com/id_g6wpwus1033w.html)
- [大模型算法工程师经典面试题————如何让 LLM 基于问题和 context 生成高质量的回答？](https://articles.zsxq.com/id_v4de86z4ck0f.html)
- [大模型算法面试题————大模型幻觉是什么，如何才能缓解这个问题？](https://articles.zsxq.com/id_6scqrpgxti1w.html)
- [大模型算法面试题————为什么 transformer 是 LayerNorm？](https://articles.zsxq.com/id_g32pqb81cc7e.html)
- [大模型算法面试题—为什么现在的主流大模型都是 decoder-only 架构？](https://articles.zsxq.com/id_0p4amgkf7pmb.html)
- [大模型算法工程师经典面试题————Attention为什么要除以根号d？”](https://articles.zsxq.com/id_tt375e8l9jf1.html)
- [大模型算法工程师经典面试题————KV Cache 原理是什么？](https://articles.zsxq.com/id_c0vn77047m40.html)
- [大模型算法工程师经典面试题————Transformers 中的 Softmax 可以并行加速么？](https://articles.zsxq.com/id_8jxx23wvz08i.html)
- [大模型算法工程师经典面试题————Transformers 中 FFN 的作用？](https://articles.zsxq.com/id_8va6lpdag8db.html)
- [大模型算法工程师经典面试题————Transformers 中的 Position Embedding 的作用？](https://articles.zsxq.com/id_180xo5ajm32m.html)
- [大模型算法工程师经典面试题————如何根据模型参数量估计需要的显存？](https://articles.zsxq.com/id_5hzvs12cs4mq.html)
- [大模型算法工程师经典面试题————为什么 Bert 的三个 Embedding 可以进行相加？](https://articles.zsxq.com/id_gxrg72dkpkvq.html)
- [【面试题】华为-交叉熵 (cross entropy) ，KL 散度的值，到底有什么含义？](https://articles.zsxq.com/id_rgtx197au4xp.html)
- [大模型面试：“分布式训练常用的通信后端都有什么？应该怎么选？”](https://articles.zsxq.com/id_cbp1ee62q8qn.html)
- [大模型算法面试题—Prompt tuning、PET、Prefix tuning、P-tuning的原理区别与代码解析一](https://articles.zsxq.com/id_kisavq75sw29.html)
- [大模型算法面试题—Prompt tuning、PET、Prefix tuning、P-tuning的原理区别与代码解析二](https://articles.zsxq.com/id_oquqmnzm0khb.html)
- [大模型算法面试题————为什么LLM推理加速有KV Cache而没有Q Cache？](https://articles.zsxq.com/id_iwlqzjowi9j1.html)
- [大模型算法面试题————LoRA 面试题汇总](https://articles.zsxq.com/id_8t3vjrrk8ws2.html)
- [大模型算法面试题————如何提升llama3训练数据质量？](https://articles.zsxq.com/id_fqweae60z059.html)
- [【面试题】阿里-Beam Search 的缺点？](https://articles.zsxq.com/id_psx3zddgwwnq.html)
- [大模型算法面试题————大模型幻觉是什么，如何才能缓解这个问题？](https://articles.zsxq.com/id_6scqrpgxti1w.html)
- [【面试题解答】解释 PPO, DPO and KTO?!](https://articles.zsxq.com/id_ud4hc4wzk8ze.html)
- [【面试题解答】问了Transformer内存优化!](https://articles.zsxq.com/id_qdwx74jlct7n.html)
- [【面试题解答】大模型预训练数据如何预处理？](https://articles.zsxq.com/id_bv2w2wwjp4w3.html)
- [【讨论】2024年，diffusion还有什么可做的?](https://articles.zsxq.com/id_een4lx29e02r.html)
- [【面试题解答】大模型中的响应延迟怎么解决？](https://articles.zsxq.com/id_1idlpap9al14.html)
- [如何确保检索到的数据是高质量的?[RAG相关]](https://wx.zsxq.com/group/51111518514854/topic/5121524228151884)
- [RAG有哪些流程，流程里各有什么优化手段?[RAG相关]](https://t.zsxq.com/7rpal)
- [PPT自动生成思路，有没有开源的好项目[AIPPT]](https://t.zsxq.com/7rpal)
- [如何计算大模型推理服务的每秒请求数（QPS）？[大模型推理性能测试]](https://t.zsxq.com/bDGZW)
- [首Token延时（TTFT）与平均输入Token数量之间存在怎样的关系？[大模型推理性能测试]](https://t.zsxq.com/bDGZW)
- [首Token延时（TTFT）与平均输入Token数量之间存在怎样的关系？[大模型推理性能测试]](https://t.zsxq.com/bDGZW)
- [在实际聊天应用中，如何估算并发用户数（VU）？[大模型推理性能测试]](https://t.zsxq.com/bDGZW)
- [介绍一下 ReAct？[agent]](https://t.zsxq.com/wPPXD)
- [解释一下 Agent Reflection 模型？[agent]](https://t.zsxq.com/wPPXD)
- [如何针对比较长的文本表格进行检索?[RAG相关]](https://t.zsxq.com/wPPXD) 
- [如何优化检索过程，以减少延迟和提高效率？ 【答案】](https://t.zsxq.com/3hvxa)
- [如何处理数据中的偏差和不一致性？ 【答案】](https://t.zsxq.com/3hvxa)
- [如何提高模型的泛化能力？ 【答案】](https://t.zsxq.com/3hvxa)
- [简答题：文章中提到了多种GPU间通信的方式，包括PCIe总线、NVLink和InfiniBand网卡。请简述这三种通信方式各自的特点以及它们通常被用于哪些场景。 【答案】](https://t.zsxq.com/TXGIy)
- [填空题：在文章中，提到了Deepspeed使用的Zero优化技术可以进一步压缩训练时显存的大小，以支持更大规模的模型训练。Zero优化技术主要通过________、________和________来减少显存占用。](https://t.zsxq.com/TXGIy)
- [介绍一下 DPO 损失函数？](https://t.zsxq.com/Td2pJ)
- [大模型 DPO 存在致命缺陷？](https://t.zsxq.com/Td2pJ)
- [大模型训练，什么时候需要预训练？什么时候需要sft？什么时候需要 dpo?](https://t.zsxq.com/Td2pJ)
- [介绍一下 Attention？](https://t.zsxq.com/NPunq)
- [传统 Attention 存在哪些问题？](https://t.zsxq.com/NPunq)
- [Attention 优化方向有哪些？](https://t.zsxq.com/NPunq)
- [介绍一下 Multi-head Attention、Grouped-query Attention、FlashAttention？](https://t.zsxq.com/NPunq)
- [有哪一些谣言检测方法推荐？](https://t.zsxq.com/NPunq)
- [大模型输出的时候带出相关文档这个功能吗？这个是后处理还是使用大模型自己总结出来的？](https://t.zsxq.com/NPunq)
- [什么是scaling test-time, 分别有哪些方法实现？](https://t.zsxq.com/pQ4xB)
- [什么是token-wise，什么是step-wise](https://t.zsxq.com/pQ4xB)
- [请描述什么是RPM, 什么是best-of-N /best-of-N-weighted/beam search RPM/LookAhead Search](https://t.zsxq.com/pQ4xB)
- [如何利用大模型做聚类吗？](https://t.zsxq.com/pQ4xB)
- [如何利用大模型自动生成长篇报告？](https://t.zsxq.com/pQ4xB)
- [embedding模型为何普遍都用encoder-only架构](https://t.zsxq.com/2daSI)
- [为什么现在的LLM都是Decoder only的架构](https://t.zsxq.com/2daSI)
- [深度学习中，批量归一化有什么好处？](https://t.zsxq.com/2daSI)
- [深度学习的batchsize必须是2的n次方吗？](https://t.zsxq.com/2daSI)
- [advanced-RAG你知道有哪些？](https://t.zsxq.com/h8m5w)
- [self-rag有哪些insight，结合工作业务场景，设计知识库问答方案？](https://t.zsxq.com/h8m5w)
- [agent和faq结合尝试过没有 这两个怎么结合啊？](https://t.zsxq.com/fk791)
- [Agent里的Memory是什么？](https://t.zsxq.com/fk791)
- [为什么需要DPO算法，DPO算法直觉与SFT算法区别，如何理论的分析？](https://t.zsxq.com/fk791)
- [如何评价RAG项目效果的好坏？](https://t.zsxq.com/Zdaou)
- [RAG 使用外挂知识库主要为了解决什么问题？](https://t.zsxq.com/Zdaou)
- [详细说说Deepspeed的机制？](https://t.zsxq.com/Zdaou)
- [大模型的幻觉问题、复读机问题是什么？](https://t.zsxq.com/irUWC)
- [为什么大模型中的响应延迟怎么解决？](https://t.zsxq.com/59DNZ)
- [大模型提速有哪些比较好的策略？](https://t.zsxq.com/59DNZ)
- [在PyTorch框架中model.train() 和 model.eval()的作用是什么？](https://t.zsxq.com/oFJeB)
- 大[模型推理时，显存中有那几部分数据？](https://t.zsxq.com/dabdPvyLHcrFhoa)
- [介绍一下 Multi-head Attention？](https://t.zsxq.com/E0JY0)
- [为什么Transformer 需要进行 Multi-head Attention？](https://t.zsxq.com/E0JY0)
- [LSTM、CNN 相对于 Self-Attention 存在什么问题？](https://t.zsxq.com/Bf9oJ)
- [介绍一下 Self-Attention？](https://t.zsxq.com/Bf9oJ)
- [为什么 Self-Attention 中 需要 除以 sqrt(dk)](https://t.zsxq.com/Bf9oJ)
- [一列数据 在不知道具体有哪些类别的情况下，如何进行分类？](https://t.zsxq.com/jYUyy)
- [现在都有哪些小于1b的中文大模型？](https://t.zsxq.com/jYUyy)
- [有大模型长文本摘要生成任务，减少露召，冗余和幻觉的办法吗？或者项目推荐？](https://t.zsxq.com/jYUyy)
- [介绍下GLU激活函数和SwiGLU激活函数？](https://t.zsxq.com/Wt4HW)
- [LLaMA1/2/3的异同？](https://t.zsxq.com/ehk0s)
- [介绍下LLaMa关键技术点？](https://t.zsxq.com/PvQ09)
- [ragflow和llamaindex区别？ 适应性？](https://t.zsxq.com/dOwHw)
- [Attention 为什么使用 Multi Head ? ](https://t.zsxq.com/piXb0)
- [提示工程中的 Zero-shot、One-shot、Multi-shot：它们到底是什么？](https://t.zsxq.com/35M6b)
- [【面试题解答】为什么transformer的FFN需要先升维再降维?](https://articles.zsxq.com/id_10wj576lo13t.html)
- [【面试题解答】字节-“大模型的参数量为什么设计成 7B，13B，33B，65B 等如此怪异的数字？”](https://articles.zsxq.com/id_mxrbwaclnsdv.html)
- [【面试题解答】model.eval() 会像 torch.no_grad() 那样停止中间激活的保存么？](https://articles.zsxq.com/id_13ic5p0eecpa.html)
- [【面试题解答】大模型上线前为什么要做推理优化？](https://articles.zsxq.com/id_ff8wtwwzjs1d.html)
- [大模型训练如何评估数据集质量](https://articles.zsxq.com/id_eguwgxzqe2vg.html)
- [【面试题】校招-"为什么 Qwen 设计成 72B？](https://t.zsxq.com/R1LfV)
- [现在大模型为什么都用 left padding？”](https://articles.zsxq.com/id_1be2sev5jbkj.html)
- [【面试题】深度学习中如何平衡多个Loss?](https://articles.zsxq.com/id_ztjrn4twppw0.html)
- [RWKV、Mamba 和 Mamba-2的核心原理和创新之处是什么？](https://articles.zsxq.com/id_td9ma6pdpc52.html)
- [介绍一下Beam Search 最坏时间复杂度是多少](https://articles.zsxq.com/id_8z4jjhcnt7da.html)
- [大模型训练框架DeepSpeed中ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3的选择](https://articles.zsxq.com/id_34g7q7ndp48r.html)
- [大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？](https://articles.zsxq.com/id_e85fjxfwqjp1.html)
- [为什么 Attention 最后采用了 Dot Product 而不是 Addition？](https://articles.zsxq.com/id_pjzc88oinl7k.html)
- [为什么DPO里Chosen和Rejected概率会同时下降???](https://articles.zsxq.com/id_rltjmqb6tp5p.html)
- [大模型算法工程师经典面试题————Flash Attention 的数学原理？](https://articles.zsxq.com/id_3aopp9kpgawv.html)
- [NLP 经典面试题————Self-Attention 的时间复杂度/空间复杂度是怎么计算的?](https://articles.zsxq.com/id_w30chh4a4t0w.html)
- [大模型算法工程师经典面试题————为什么 output token 的价格比 input token 更贵？](https://articles.zsxq.com/id_4v092042xwjk.html)
- [大模型算法工程师经典面试题————反向传播的计算量是前向传播计算量的几倍？](https://articles.zsxq.com/id_0x552s49nbdr.html)
- [大模型算法面试题————为什么LLM要用Mask？](https://articles.zsxq.com/id_0gzg6f84hwf8.html)
- [大模型算法工程师经典面试题————大模型如何才能处理更长的文本？](https://articles.zsxq.com/id_ntcp0xon3n56.html)
- [阿里面试官问：如何根据大模型大小计算所需显存？](https://articles.zsxq.com/id_7ber6sh5prgc.html)
- [大模型算法工程师经典面试题————共享权重如何求梯度？](https://articles.zsxq.com/id_wu9id9ngmfso.html)
- [激活函数在神经网络中有什么用？](https://articles.zsxq.com/id_z6chfe1l6kdd.html)
- [大模型常用的 Normalization 都有什么？](https://articles.zsxq.com/id_gqswkgp2wekl.html)
- [【面试题】大模型推理的时候 top k 和 top p 同时设置的时候怎么采样？](https://articles.zsxq.com/id_gv4gt882cwl7.html)
- [给一些 token id 和 对应的 tokenizer， 可以将其无损的还原为原始文本么？](https://articles.zsxq.com/id_3c4i8te035eq.html)
- [【面试题】字节一面：大模型如何评测以及当前测评的困境](https://articles.zsxq.com/id_l6ovbio4tzqh.html)


- [算法岗面试中，C++高频题，真的很有 用](https://articles.zsxq.com/id_zjw9twlrmm2v.html)

前馈层
